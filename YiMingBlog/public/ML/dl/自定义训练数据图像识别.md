---
layout: doc
title: 自定义训练数据图像识别
tags:
  - "#博文"
  - "#以撒"
  - "#深度学习"
  - "#图像识别"
datetime: 2024-01-23
time: 14:37
description: 
navbar: true
sidebar: true
footer: true
---

# 引言
![[3.png]]
最近在玩《以撒的结合:忏悔》, ns版本. 游戏确实上头好玩, 内容很多. 但是游玩下来遇到一个纠结头疼的问题就是, 道具/ 装备 有时候捡起来后的效果, 还不如不捡.   
装备/道具只会在捡起来的时候才能看到道具和状态是什么, 甚至, 有时候捡起来后描述也看不出这个道具到底能用来干嘛,  魂系叙事那一套......虽然也找到不错的[以撒wiki]([以撒的结合中文维基 - 灰机wiki - 北京嘉闻杰诺网络科技有限公司 (huijiwiki.com)](https://isaac.huijiwiki.com/wiki/%E9%A6%96%E9%A1%B5)), 但是, 站点只能文字搜索, 再加上其道具数量有7百多个, 叠加起来就更让人头疼了. 
![[以撒自定义训练数据图像识别-2024-01-23.png]]

于是就想到, 干脆做一个“以撒道具图像识别功能” 好了, 优化体验: ) .   

# 自定义训练数据图像识别

得益于各种深度学习框架的发展, 现在训练一个模型变得非常简单, 于是我就决定直接用`pytorch`来完成这个图片识别的任务了.  考虑到以撒的道具装备这类东西图像数据是非常**垂直领域**了, 所以我需要准备对应的训练数据进行模型的训练.  

## 数据准备  

### 数据获取&数据合成  
从[以撒的结合中文维基](https://isaac.huijiwiki.com/wiki/%E9%A6%96%E9%A1%B5) 编写爬虫把道具/装备的图片数据, 描述都抓取下来.  
爬虫比较简单, 主要就是图片爬取的时候, 需要从css中把sprite图子图的x, y提取出来, 然后自行使用sprite划分独立的道具图. 下图为css和html标签中抽取出来的信息
```python
{'collectibles_001': {'x': 32,
  'y': 0,
  'en': 'The Sad Onion',
  'zh': '悲伤洋葱',
  'level': '1',
  'type': '道具',
  'level2': '3',
  '?': '/',
  'desc': '射速上升。',
  'new_id': 0,
  'image': './cus_data/0.png'}, ...
} # 提取出每个道具的图片和其他属性
```

然后把图片分割后某个目录, 此处我是把裁切好的图片存放在 `./cus_data`下. 并且把图片. 整理后得到训练的类别标签备用.
```python
>>>classes
{0: '悲伤洋葱:射速上升。',
 1: '内眼:角色每次发射3颗泪弹。',
 2: '弯勺魔术:角色的泪弹获得追踪效果。',
 3: '柯吉猫的头:泪弹变大，击退效果上升，伤害上升。',
 4: '我的镜像:角色的泪弹会飞回角色身边。',
 5: '小号:射程下降，射速上升。',
 ...
```

此处我把图片缩放成64 * 64 的格式后保存, 模拟真实使用相机拍摄后扣取的大小范围.
![[自定义训练数据图像识别-2024-01-24.png]]
### 数据增强  

考虑到实际的预测情况都伴随着各种各样的背景干扰,  所以这边需要在合成训练数据的时候, 也需要让这些元图标拥有各种各样的背景.

另一方面, 考虑到一个场景就是使用摄像头的拍照的时候, 可能会有的一些图像的畸变和变形等. transform中也需要配置数据增强的操作, 包括但不限于, 抖动, 明暗度变化等.
```python
from torchvision.transforms import ToTensor

transform = transforms.Compose(
    [
        transforms.Lambda(lambda x: x.float()),
    ]) # 先直接配置一个用来计算 数据均值和标准差,以方便正则化

# 配置初始的dataloader用于遍历和计算均值和标准差
from torchvision.io import read_image

class IssacCustomDatasets(Dataset):
    def __init__(self, img_sort_files, 
                 img_dir, transform=None, 
                target_transform=None):
        self.img_labels = img_sort_files
        # 自定义标签关系, 此处需要排好序的
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform
        
    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):  # 作用是获得label 和 item 即可
        filename = self.img_labels[idx]        
        img_path = os.path.join(self.img_dir, filename)
        image = read_image(img_path, mode=torchvision.io.image.ImageReadMode.RGB)

        label = int(filename.split(".")[0])
        if self.transform:
            image = self.transform(image)
        if self.target_transform:
            label = self.target_transform(label)
        return image, label

from_dir = "new_cus_data"
need_move_images = os.listdir("new_cus_data/")
need_move_images = [i for i in need_move_images if int(i.split(".")[0]) ]
need_move_images.sort(key=lambda x: f"{int(x.split('.')[0]):03d}" + f"{x.split('.')[1]}")
print(len(need_move_images))

train_dataset = IssacCustomDatasets(img_sort_files=need_move_images, img_dir="new_cus_data/",
                                    transform=transform)

## dataloader
train_dataloader = DataLoader(train_dataset, 
                              batch_size=batch_size, 
                              shuffle=True)

# 获取图片数据的 归一化数值
global_mean = []
global_std = []
for images, labels in train_dataloader:
    numpy_image = images.numpy()
    batch_mean = np.mean(numpy_image, axis=(0,2,3))
    batch_std = np.std(numpy_image, axis=(0,2,3))
    global_mean.append(batch_mean)
    global_std.append(batch_std)

global_mean = np.mean(global_mean, axis=0).tolist()
global_std = np.mean(global_std, axis=0).tolist()
print(global_mean)
print(global_std)
```

再次根据得到的均值和标准差更新训练用的dataloader
```python
from torchvision.transforms import ToTensor
        
transform = transforms.Compose(
    [
        transforms.RandomAffine(degrees=0, translate=None, scale=(0.9, 1.1), shear=None),  
        # 随机放大或者缩小一点点
        # 增加噪声, 防止过拟合
        transforms.ColorJitter(
            brightness=0.1,
            contrast=0.1,
            saturation=0.1,
            hue=0.1),  # 抖动图像的亮度、对比度、饱和度和色相
        transforms.Lambda(lambda x: x.float()),
        transforms.Normalize(
            global_mean,
            global_std
        )  # 对图片数据做正则化
    ])

train_dataset = IssacCustomDatasets(need_move_images, img_dir="new_cus_data/", transform=transform)

## dataloader
train_dataloader = DataLoader(train_dataset, 
                              batch_size=batch_size, 
                              shuffle=True)
```

## 训练模型  
### 自定义模型  
因为图片识别任务输入的图片比较简单, 所以我这儿直接用vgg16来训练一个小模型就可以了. 考虑到原版的vgg16是基于244x244的ImageNet的图片输入, 此处我还需要调整一下他最后池化层的7x7的特征图的输入, 还有最后的全连接层的输出.
```python
import torch
from torch import nn
from torchvision.models import vgg16
import torch.optim as optim
import numpy as np

device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)
print(f"Using {device} device")

total_classes = list(set([i.split(".")[0] for i in need_move_images]))
total_classes_num = len(total_classes)

## 大批量测试
from matplotlib import pyplot as plt
from torchvision.utils import make_grid

class VGG16_S(nn.Module):
    def __init__(self, num_classes):
        super(VGG16_S, self).__init__()
        model = vgg16(pretrained=True)  # 可以选择False 和True 
        self.features = model.features  # 只取了feature
        self.classifier = nn.Sequential(
            nn.Linear(512 * 2 * 2, 4096),  # 修改此处的第一个参数用来适配64*64
            nn.ReLU(True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(True),
            nn.Dropout(),
            nn.Linear(4096, num_classes),
        )
        
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1) 
        x = self.classifier(x)
        return x
      
```

### 训练识别模型    
使用多个不同背景合成的训练数据和数据增强后, 我们获得了 8000+的总训练数据, 700多个类. 然后就可以进行相关的训练了. (数据量实在不算太多, 所以偷懒直接回抽样训练数据验证好了)

```python
%%time
# 训练模型

# 如果不根据之前的进行训练就读取这个
net = VGG16_S(num_classes=total_classes_num).to(device)  # 这次训练64 * 64的版本

criterion = nn.CrossEntropyLoss()

optimizer = optim.Adam(net.parameters(), lr=0.0001, weight_decay=0.01)  # 加入了l2正则化

check_iter = 10 # train check batch size 
train_epoch = 25
prefix = "20240127_full_64x64_clear_718class_l2_"

for epoch in range(train_epoch):
    net.train()  # 每个epoch 后切换训练模式, 那么会不会保d留之前的训练权重呢?
    
    progress_bar = tqdm(enumerate(train_dataloader, 0), total=len(train_dataloader))
    for i, data in progress_bar:
        inputs, labels = data
        inputs, labels = inputs.float().to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        progress_bar.set_description(f'epoch:{epoch}, loss: {loss / batch_size:.3f}')
    
    if epoch % 3 == 0:  # 每两个epoch进行一次验证
        print("开始验证....")
        net.eval()
        correct = 0  # 记录正确预测的数量
        total = 0  # 总的样本数
        with torch.no_grad():
            
            progress_bar2 = tqdm(enumerate(train_dataloader, 0), total=int(len(train_dataloader) * 0.2))
            for i, data in progress_bar2:
                if i >= int(len(train_dataloader) * 0.2):
                    break
                inputs, labels = data
                inputs, labels = inputs.float().to(device), labels.to(device)
                outputs = net(inputs)
                _, predicted = torch.max(outputs.data, 1)

                # 统计准确率
                total += labels.size(0)  # 实际的样本数
                correct += (predicted == labels).sum().item()  # 正确预测的样本数
        accuracy = round(correct / total * 100, 3)  # 计算准确率
        print(f'epoch:{epoch}, Accuracy: {accuracy:.3f}')

        save_model_path = f"{prefix}_vgg16_{epoch}_{accuracy}.pth"
        torch.save(net.state_dict(), save_model_path)
        print(f"model save: ", save_model_path)
        
    torch.cuda.empty_cache()
print('Finished Training')

save_model_path = f"{prefix}_vgg16_{epoch}_{accuracy}.pth"
torch.save(net.state_dict(), save_model_path)
print(f"model save: ", save_model_path)
```

## 验证训练成果  
最终25epoch训练以后, 得到了一个92%的准确率的结果, 这之后我还让他训练了额外的10轮, 但是准确率都没有再提升, 且增加了过拟合的风险,所以这边我就去25轮的最终模型结果了. 下面为模型的最终输出. 

```
epoch:24, Accuracy: 92.196  
model save: 20240127_full_64x64_clear_718class_l2__vgg16_24_92.196.pth  
Finished Training  
model save: 20240127_full_64x64_clear_718class_l2__vgg16_24_92.196.pth  
CPU times: user 4h 41min 37s, sys: 28min 20s, total: 5h 9min 57s  
Wall time: 1h 19min 47s
```

读取模型, 并且使用手机拍照的截图, 进行真实真实图片验证:
```python

model_path = "20240127_full_64x64_clear_718class_l2__vgg16_24_92.196.pth"

model = VGG16_S(num_classes=total_classes_num).to(device)  # 这次训练64 * 64的版本
model.load_state_dict(torch.load(model_path))  # 不用百分百的

def get_real_label(class_index):
    print(class_index, 
          new_dict[class_index]['zh'], 
          new_dict[class_index]['en'],
          new_dict[class_index]['desc'])
    return new_dict[class_index]

def eval_predict(model, image_path):
    image = read_image(image_path, mode=torchvision.io.image.ImageReadMode.RGB)
    inner_transform = transforms.Compose(
    [
        transforms.Resize((64, 64)),  # 确保模型的输入是64*64
        transforms.ColorJitter(
            brightness=0.1,
            contrast=0.1,
            saturation=0.1,
            hue=0.1),  # 抖动图像的亮度、对比度、饱和度和色相
        transforms.Lambda(lambda x: x.float()),
        transforms.Normalize(
            global_mean,
            global_std
        )  # 对图片数据做正则化
    ])
    
    plt.imshow(np.transpose(image, (1, 2, 0)))
    plt.show()
    model.eval()
    timg = inner_transform(image)
    timg = timg.to(device)
    timg1 = timg.unsqueeze(0)
    # 输入的图片需要改成64 * 64
    result = model(timg1)
    result
    _, predicted = torch.max(result, 1)
    get_real_label(predicted.item())
    return result, predicted
```

图片验证:  
推理, 并且得到前10个最像的类型, 优化用户体验. 因为训练数据有限, 所以才用了增加找回的形式给用户选择.毕竟,本质是方便用户识别道具类型.  
```python
import torch.nn.functional as F

real_test_images = [i for i in os.listdir("cus_test_data/") if i.find(".") != 0]
print(len(real_test_images))

for t_image_path in real_test_images:
    print("real label: ", image_path.split("/")[-1])    
    
    t_image_path = os.path.join("cus_test_data", t_image_path)
    t_image = Image.open(t_image_path)
    t_image = t_image.resize((64, 64))
    result, predicted = eval_predict(model, t_image_path)

    probabilities = F.softmax(result, dim=1)  

    values, indices = torch.topk(probabilities, 10) # 使用topk获取
    match_images = [t_image]
    for p, i in zip(values[0].tolist(), indices[0].tolist()):
        data = get_real_label(i)
        print(f"{data['en']} {data['zh']}: {round(100*p, 2)}%")

        match_images.append(Image.open(data.get("image")))
        
    plot_images(match_images)
    print()

    print("----------------------------------------------")
#     break
```
![[自定义训练数据图像识别-2024-01-27.png]]
预测结果:  

![[自定义训练数据图像识别预测结果-2024-01-28.png]]

![[自定义训练数据图像识别预测结果-2024-01-28-1.png]]
# 总结  

一时兴起的小需求, 也很多小细节需要处理. 
大批量数据运行前, 需要先抽少一些类进行小批量的快速验证, 对模型和训练数据进行验证. 这次我就是先用20类进行验证, 之后才进行700多类的训练.   

一开始用vgg, 用的是224x224的图片输入, 后面为了提速.因为是像素画,只要不要平滑, 64x64也足够清晰, 所以就调整vgg结构, 方便进行增加更多的batch_size进行更快的训练. (原图是32x32的, 如果要更极限一点, 全部用32x32,可以更快,  没验证了 : )  


下一步就是部署到移动端, 毕竟玩游戏, 就是希望能够方便使用 :) 

[完整代码](https://github.com/realzhengyiming/isaac_tools/blob/main/simple_issac_item_recognition-%E5%85%A8%E9%87%8F%E8%AE%AD%E7%BB%83%E7%9A%84%E7%89%88%E6%9C%AC.ipynb)在github上了   

我要去玩游戏了, 以撒真好玩 : ) 

## 后续优化    
+ 增加一下真实的截图作为训练数据, 以进一步提高各个类别的准确度
+ 进一步增加一些其他的数据增强, 以更好的模拟真实的使用情况
+ 考虑换用其他模型  