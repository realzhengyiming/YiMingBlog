---
title: ä»¥æ’’é“å…·/é¥°å“å›¾åƒè¯†åˆ«
time: 14:37
description: è‡ªå®šä¹‰è®­ç»ƒæ•°æ®çš„ä»¥æ’’é“å…·å›¾åƒè¯†åˆ«, ã€Šä»¥æ’’çš„ç»“åˆã€‹è®­ç»ƒæ•°æ®é“å…·è£…å¤‡è¯†åˆ«
navbar: true
sidebar: true
footer: true
date: 2024-01-29
category: Tutorial
author: Yi Ming
next: true
tags:
  - æ·±åº¦å­¦ä¹ 
  - å›¾åƒè¯†åˆ«
  - ä»¥æ’’çš„ç»“åˆ
---

# å¼•è¨€  

![](pic/è‡ªå®šä¹‰è®­ç»ƒæ•°æ®å›¾åƒè¯†åˆ«-2024-01-29.png)
æœ€è¿‘åœ¨ç©ã€Šä»¥æ’’çš„ç»“åˆ:å¿æ‚”ã€‹, nsç‰ˆæœ¬. æ¸¸æˆç¡®å®ä¸Šå¤´å¥½ç©, å†…å®¹å¾ˆå¤š. ä½†æ˜¯æ¸¸ç©ä¸‹æ¥é‡åˆ°ä¸€ä¸ªçº ç»“å¤´ç–¼çš„é—®é¢˜å°±æ˜¯, é“å…·/ è£…å¤‡ æœ‰æ—¶å€™æ¡èµ·æ¥åçš„æ•ˆæœ, è¿˜ä¸å¦‚ä¸æ¡. è£…å¤‡/é“å…·åªä¼šåœ¨æ¡èµ·æ¥çš„æ—¶å€™æ‰èƒ½çœ‹åˆ°é“å…·å’ŒçŠ¶æ€æ˜¯ä»€ä¹ˆ, ç”šè‡³, æœ‰æ—¶å€™æ¡èµ·æ¥åæè¿°ä¹Ÿçœ‹ä¸å‡ºè¿™ä¸ªé“å…·åˆ°åº•èƒ½ç”¨æ¥å¹²å˜›,  é­‚ç³»å™äº‹é‚£ä¸€å¥—......è™½ç„¶ä¹Ÿæ‰¾åˆ°ä¸é”™çš„[ä»¥æ’’çš„ç»“åˆä¸­æ–‡ç»´åŸº](https://isaac.huijiwiki.com/wiki/%E9%A6%96%E9%A1%B5), ä½†æ˜¯, ç«™ç‚¹åªèƒ½æ–‡å­—æœç´¢, å†åŠ ä¸Šå…¶é“å…·å’Œé¥°å“åŠ èµ·æ¥æ€»æ•°é‡æœ‰900å¤šä¸ª, ç›¸å½“äºImageNetçš„ç§ç±»äº†. 
äºæ˜¯å°±æƒ³åˆ°, å¹²è„†åšä¸€ä¸ªâ€œä»¥æ’’é“å…·å›¾åƒè¯†åˆ«åŠŸèƒ½â€ å¥½äº†, ä¼˜åŒ–ä½“éªŒ: ) .   


---  


# è‡ªå®šä¹‰è®­ç»ƒæ•°æ®å›¾åƒè¯†åˆ«

å¾—ç›Šäºå„ç§æ·±åº¦å­¦ä¹ æ¡†æ¶çš„å‘å±•, ç°åœ¨è®­ç»ƒä¸€ä¸ªæ¨¡å‹å˜å¾—éå¸¸ç®€å•, äºæ˜¯æˆ‘å°±å†³å®šç›´æ¥ç”¨`pytorch`æ¥å®Œæˆè¿™ä¸ªå›¾ç‰‡è¯†åˆ«çš„ä»»åŠ¡äº†.  è€ƒè™‘åˆ°ä»¥æ’’çš„é“å…·è£…å¤‡è¿™ç±»ä¸œè¥¿å›¾åƒæ•°æ®æ˜¯éå¸¸**å‚ç›´é¢†åŸŸ**äº†, æ‰€ä»¥æˆ‘éœ€è¦å‡†å¤‡å¯¹åº”çš„è®­ç»ƒæ•°æ®è¿›è¡Œæ¨¡å‹çš„è®­ç»ƒ.  
# æ•°æ®å‡†å¤‡   

## æ•°æ®è·å–&æ•°æ®åˆæˆ   

ä»[ä»¥æ’’çš„ç»“åˆä¸­æ–‡ç»´åŸº](https://isaac.huijiwiki.com/wiki/%E9%A6%96%E9%A1%B5) ç¼–å†™çˆ¬è™«æŠŠé“å…·/è£…å¤‡çš„å›¾ç‰‡æ•°æ®, æè¿°éƒ½æŠ“å–ä¸‹æ¥.  
çˆ¬è™«æ¯”è¾ƒç®€å•, ä¸»è¦å°±æ˜¯å›¾ç‰‡çˆ¬å–çš„æ—¶å€™, éœ€è¦ä»cssä¸­æŠŠspriteå›¾å­å›¾çš„x, yæå–å‡ºæ¥, ç„¶åè‡ªè¡Œä½¿ç”¨spriteåˆ’åˆ†ç‹¬ç«‹çš„é“å…·å›¾. ä¸‹å›¾ä¸ºcsså’Œhtmlæ ‡ç­¾ä¸­æŠ½å–å‡ºæ¥çš„ä¿¡æ¯
```python
{'collectibles_001': {'x': 32,
  'y': 0,
  'en': 'The Sad Onion',
  'zh': 'æ‚²ä¼¤æ´‹è‘±',
  'level': '1',
  'type': 'é“å…·',
  'level2': '3',
  '?': '/',
  'desc': 'å°„é€Ÿä¸Šå‡ã€‚',
  'new_id': 0,
  'image': './cus_data/0.png'}, ...
} # æå–å‡ºæ¯ä¸ªé“å…·çš„å›¾ç‰‡å’Œå…¶ä»–å±æ€§
```

ç„¶åæŠŠå›¾ç‰‡åˆ†å‰²åæŸä¸ªç›®å½•, æ­¤å¤„æˆ‘æ˜¯æŠŠè£åˆ‡å¥½çš„å›¾ç‰‡å­˜æ”¾åœ¨ `./cus_data`ä¸‹. å¹¶ä¸”æŠŠå›¾ç‰‡. æ•´ç†åå¾—åˆ°è®­ç»ƒçš„ç±»åˆ«æ ‡ç­¾å¤‡ç”¨.
```python
>>>classes
{0: 'æ‚²ä¼¤æ´‹è‘±:å°„é€Ÿä¸Šå‡ã€‚',
 1: 'å†…çœ¼:è§’è‰²æ¯æ¬¡å‘å°„3é¢—æ³ªå¼¹ã€‚',
 2: 'å¼¯å‹ºé­”æœ¯:è§’è‰²çš„æ³ªå¼¹è·å¾—è¿½è¸ªæ•ˆæœã€‚',
 3: 'æŸ¯å‰çŒ«çš„å¤´:æ³ªå¼¹å˜å¤§ï¼Œå‡»é€€æ•ˆæœä¸Šå‡ï¼Œä¼¤å®³ä¸Šå‡ã€‚',
 4: 'æˆ‘çš„é•œåƒ:è§’è‰²çš„æ³ªå¼¹ä¼šé£å›è§’è‰²èº«è¾¹ã€‚',
 5: 'å°å·:å°„ç¨‹ä¸‹é™ï¼Œå°„é€Ÿä¸Šå‡ã€‚',
 ...
```

æ­¤å¤„æˆ‘æŠŠå›¾ç‰‡è£å‰ªä¸‹æ¥çš„32x32å¤§å°çš„å›¾ç‰‡ä¿å­˜ä¸ºåˆæˆè®­ç»ƒæ•°æ®çš„å…ƒæ•°æ®, å› ä¸ºæ˜¯åƒç´ é£æ ¼, æ‰€ä»¥å“ªæ€•æ‰€ç¼©å°äº†,åªè¦ä¸ä½¿ç”¨å¹³æ»‘, é‚£è¿˜æ˜¯éå¸¸æ¸…æ™°çš„.
![](pic/è‡ªå®šä¹‰è®­ç»ƒæ•°æ®å›¾åƒè¯†åˆ«-åƒç´ é£æ ¼-2024-01-29.png)  
(æ•°æ®è·å¾—å’Œå¤„ç†å®Œæ•´ä»£ç åœ¨æœ«å°¾çš„å¼•ç”¨ä¸­ä¼šç»™å‡ºé“¾æ¥.)
## æ•°æ®å¢å¼º   

å®é™…çš„é¢„æµ‹æƒ…å†µéƒ½ä¼´éšç€å„ç§å„æ ·çš„èƒŒæ™¯å¹²æ‰°,  æ‰€ä»¥è¿™è¾¹éœ€è¦åœ¨åˆæˆè®­ç»ƒæ•°æ®çš„æ—¶å€™, ä¹Ÿéœ€è¦è®©è¿™äº›å…ƒå›¾æ ‡æ‹¥æœ‰å„ç§å„æ ·çš„èƒŒæ™¯.
![](pic/è‡ªå®šä¹‰è®­ç»ƒæ•°æ®å›¾åƒè¯†åˆ«-æ•°æ®åˆæˆå¤šèƒŒæ™¯-2024-01-29.png)
å¦ä¸€æ–¹é¢, è€ƒè™‘åˆ°ä¸€ä¸ªåœºæ™¯å°±æ˜¯ä½¿ç”¨æ‘„åƒå¤´çš„æ‹ç…§çš„æ—¶å€™, å¯èƒ½ä¼šæœ‰çš„ä¸€äº›å›¾åƒçš„ç•¸å˜å’Œå˜å½¢ç­‰. transformä¸­ä¹Ÿéœ€è¦é…ç½®æ•°æ®å¢å¼ºçš„æ“ä½œ, åŒ…æ‹¬ä½†ä¸é™äº, æŠ–åŠ¨, æ˜æš—åº¦å˜åŒ–ç­‰.
```python
from torchvision.transforms import ToTensor

transform = transforms.Compose(
    [
        transforms.Lambda(lambda x: x.float()),
    ]) # å…ˆç›´æ¥é…ç½®ä¸€ä¸ªç”¨æ¥è®¡ç®— æ•°æ®å‡å€¼å’Œæ ‡å‡†å·®,ä»¥æ–¹ä¾¿æ­£åˆ™åŒ–

# é…ç½®åˆå§‹çš„dataloaderç”¨äºéå†å’Œè®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
from torchvision.io import read_image

class IssacCustomDatasets(Dataset):
    def __init__(self, img_sort_files, 
                 img_dir, transform=None, 
                target_transform=None):
        self.img_labels = img_sort_files
        # è‡ªå®šä¹‰æ ‡ç­¾å…³ç³», æ­¤å¤„éœ€è¦æ’å¥½åºçš„
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform
        
    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):  # ä½œç”¨æ˜¯è·å¾—label å’Œ item å³å¯
        filename = self.img_labels[idx]        
        img_path = os.path.join(self.img_dir, filename)
        image = read_image(img_path, mode=torchvision.io.image.ImageReadMode.RGB)

        label = int(filename.split(".")[0])
        if self.transform:
            image = self.transform(image)
        if self.target_transform:
            label = self.target_transform(label)
        return image, label

from_dir = "new_cus_data"
need_move_images = os.listdir("new_cus_data/")
need_move_images = [i for i in need_move_images if int(i.split(".")[0]) ]
need_move_images.sort(key=lambda x: f"{int(x.split('.')[0]):03d}" + f"{x.split('.')[1]}")
print(len(need_move_images))

train_dataset = IssacCustomDatasets(img_sort_files=need_move_images, img_dir="new_cus_data/",
                                    transform=transform)

## dataloader
train_dataloader = DataLoader(train_dataset, 
                              batch_size=batch_size, 
                              shuffle=True)

# è·å–å›¾ç‰‡æ•°æ®çš„ å½’ä¸€åŒ–æ•°å€¼
global_mean = []
global_std = []
for images, labels in train_dataloader:
    numpy_image = images.numpy()
    batch_mean = np.mean(numpy_image, axis=(0,2,3))
    batch_std = np.std(numpy_image, axis=(0,2,3))
    global_mean.append(batch_mean)
    global_std.append(batch_std)

global_mean = np.mean(global_mean, axis=0).tolist()
global_std = np.mean(global_std, axis=0).tolist()
print(global_mean)
print(global_std)
```

å†æ¬¡æ ¹æ®å¾—åˆ°çš„å‡å€¼å’Œæ ‡å‡†å·®æ›´æ–°è®­ç»ƒç”¨çš„dataloader
```python
from torchvision.transforms import ToTensor
        
transform = transforms.Compose(
    [
        transforms.RandomAffine(degrees=0, translate=None, scale=(0.9, 1.1), shear=None),  
        # éšæœºæ”¾å¤§æˆ–è€…ç¼©å°ä¸€ç‚¹ç‚¹
        # å¢åŠ å™ªå£°, é˜²æ­¢è¿‡æ‹Ÿåˆ
        transforms.ColorJitter(
            brightness=0.1,
            contrast=0.1,
            saturation=0.1,
            hue=0.1),  # æŠ–åŠ¨å›¾åƒçš„äº®åº¦ã€å¯¹æ¯”åº¦ã€é¥±å’Œåº¦å’Œè‰²ç›¸
        transforms.Lambda(lambda x: x.float()),
        transforms.Normalize(
            global_mean,
            global_std
        )  # å¯¹å›¾ç‰‡æ•°æ®åšæ­£åˆ™åŒ–
    ])

train_dataset = IssacCustomDatasets(need_move_images, img_dir="new_cus_data/", transform=transform)

## dataloader
train_dataloader = DataLoader(train_dataset, 
                              batch_size=batch_size, 
                              shuffle=True)
```

# è®­ç»ƒæ¨¡å‹  

## è‡ªå®šä¹‰æ¨¡å‹   

å› ä¸ºå›¾ç‰‡è¯†åˆ«ä»»åŠ¡è¾“å…¥çš„å›¾ç‰‡æ¯”è¾ƒç®€å•, æ‰€ä»¥æˆ‘è¿™å„¿ç›´æ¥ç”¨vgg16æ¥è®­ç»ƒä¸€ä¸ªå°æ¨¡å‹å°±å¯ä»¥äº†. åŸç‰ˆçš„vgg16æ˜¯åŸºäº244x244çš„ImageNetçš„å›¾ç‰‡è¾“å…¥, æ­¤å¤„æˆ‘é€šè¿‡ç»§æ‰¿è°ƒæ•´äº†æ± åŒ–å±‚çš„7x7çš„ç‰¹å¾å›¾çš„è¾“å…¥, ä»¥è®©ä»–æ”¯æŒ32x32çš„è¾“å…¥è¿˜æœ‰æœ€åçš„å…¨è¿æ¥å±‚çš„è¾“å‡º.
```python
import torch
from torch import nn
from torchvision.models import vgg16
import torch.optim as optim
import numpy as np

device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)
print(f"Using {device} device")

total_classes = list(set([i.split(".")[0] for i in need_move_images]))
total_classes_num = len(total_classes)

## å¤§æ‰¹é‡æµ‹è¯•
from matplotlib import pyplot as plt
from torchvision.utils import make_grid

class VGG16_S(nn.Module):
    def __init__(self, num_classes):
        super(VGG16_S, self).__init__()
        model = vgg16(pretrained=True)  # å¯ä»¥é€‰æ‹©False å’ŒTrue 
        self.features = model.features  # åªå–äº†feature
        self.classifier = nn.Sequential(
            nn.Linear(512 * 1 * 1, 4096),  # ä¿®æ”¹æ­¤å¤„çš„ç¬¬ä¸€ä¸ªå‚æ•°ç”¨æ¥é€‚é…64*64
            nn.ReLU(True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(True),
            nn.Dropout(),
            nn.Linear(4096, num_classes),
        )
        
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1) 
        x = self.classifier(x)
        return x
      
```

## è®­ç»ƒè¯†åˆ«æ¨¡å‹    

ä½¿ç”¨å¤šä¸ªä¸åŒèƒŒæ™¯åˆæˆçš„è®­ç»ƒæ•°æ®å’Œæ•°æ®å¢å¼ºå, æˆ‘ä»¬è·å¾—äº† 10872 çš„æ€»è®­ç»ƒæ•°æ®, 921ä¸ªç±». ç„¶åå°±å¯ä»¥è¿›è¡Œç›¸å…³çš„è®­ç»ƒäº†. (æ•°æ®é‡å®åœ¨ä¸ç®—å¤ªå¤š, æ‰€ä»¥å·æ‡’ç›´æ¥å›æŠ½æ ·è®­ç»ƒæ•°æ®éªŒè¯å¥½äº†)

```python
%%time
# è®­ç»ƒæ¨¡å‹

net = VGG16_S(num_classes=total_classes_num).to(device)  # è¿™æ¬¡è®­ç»ƒ64 * 64çš„ç‰ˆæœ¬

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.0001, weight_decay=0.01)  # åŠ å…¥äº†l2æ­£åˆ™åŒ–

check_iter = 10 # train check batch size 
train_epoch = 25
prefix = f"20240128_full_32x32_clear_{total_classes_num}class_l2_2-"

print("batch_size", batch_size)
for epoch in range(train_epoch):
    net.train()  # æ¯ä¸ªepoch ååˆ‡æ¢è®­ç»ƒæ¨¡å¼, é‚£ä¹ˆä¼šä¸ä¼šä¿dç•™ä¹‹å‰çš„è®­ç»ƒæƒé‡å‘¢?
    
    progress_bar = tqdm(enumerate(train_dataloader, 0), total=len(train_dataloader))
    for i, data in progress_bar:
        inputs, labels = data  # å¿…é¡»è¦float å½’ä¸€åŒ–? æµ®ç‚¹ç±»å‹.
        inputs, labels = inputs.float().to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = net(inputs)  # è¿™ä¸€æ­¥, è¿è¡Œæœ‰é—®é¢˜, è¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢, æ£€æŸ¥ä¸€ä¸‹å›¾ç‰‡æ ¼å¼

        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        progress_bar.set_description(f'epoch:{epoch}, loss: {loss / batch_size:.3f}')
    
    if epoch % 1 == 0:  # æ¯ä¸¤ä¸ªepochè¿›è¡Œä¸€æ¬¡éªŒè¯
        print("å¼€å§‹éªŒè¯....")
        net.eval()
        correct = 0  # è®°å½•æ­£ç¡®é¢„æµ‹çš„æ•°é‡
        total = 0  # æ€»çš„æ ·æœ¬æ•°
        with torch.no_grad():
            
            progress_bar2 = tqdm(enumerate(train_dataloader, 0), total=int(len(train_dataloader) * 0.4))
            for i, data in progress_bar2:
                if i >= int(len(train_dataloader) * 0.4):
                    break
                inputs, labels = data
                inputs, labels = inputs.float().to(device), labels.to(device)
                outputs = net(inputs)
                _, predicted = torch.max(outputs.data, 1)

                total += labels.size(0)  # å®é™…çš„æ ·æœ¬æ•°
                correct += (predicted == labels).sum().item()  # æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬æ•°
        
        accuracy = round(correct / total * 100, 3)  # è®¡ç®—å‡†ç¡®ç‡
        print(f'epoch: {epoch}, Accuracy: {accuracy:.3f}')
        if accuracy >= 60:  # å¤ªå°çš„æ ¹æœ¬æ²¡æ¯”è¾ƒä¿å­˜
            save_model_path = f"{prefix}_vgg16_{epoch}_{accuracy}.pth"
            torch.save(net.state_dict(), save_model_path)
            print(f"model save: ", save_model_path)
        
    torch.cuda.empty_cache()
print('Finished Training')

save_model_path = f"{prefix}_vgg16_{epoch}_{accuracy}.pth"
torch.save(net.state_dict(), save_model_path)
print(f"model save: ", save_model_path)
```

# éªŒè¯è®­ç»ƒæˆæœ   

ç»è¿‡å¤šæ¬¡è®­ç»ƒéªŒè¯, æœ€ç»ˆè®¾ç½®25epoch, æ¯è½®ä¿å­˜ç»“æœå, å¾—åˆ°äº†å¤šä¸ª90ä»¥ä¸Šå‡†ç¡®ç‡çš„ç»“æœ, è¿™ä¹‹åæˆ‘è¿˜è®©ä»–è®­ç»ƒäº†é¢å¤–çš„10è½®, ä½†æ˜¯å‡†ç¡®ç‡éƒ½æ²¡æœ‰å†æå‡,åè€Œçªä¸‹é™äº†, è¯´æ˜åç»­æ›´å¤šçš„epochå°±æ˜¯è¿‡æ‹Ÿåˆäº†, ä¸éœ€è¦å†å¢åŠ è½®æ•°.  
æœ€ç»ˆæˆ‘æ‰ç”¨äº†16 epochçš„æ¨¡å‹ç»“æœ: 
```
epoch: 16, Accuracy: 98.001
model save:  20240128_full_32x32_clear_906class_l2_2-_vgg16_16_98.001.pth
```

## éªŒè¯ä»£ç    

è¯»å–æ¨¡å‹, å¹¶ä¸”ä½¿ç”¨æ‰‹æœºæ‹ç…§çš„æˆªå›¾, å¹¶ä½¿ç”¨çœŸå®å›¾ç‰‡è¿›è¡ŒéªŒè¯:
```python

model_path = "20240128_full_32x32_clear_906class_l2_2-_vgg16_16_98.001.pth"  # è¿™ä¸ªæ•ˆæœä¹Ÿä¸é”™ 19, çœ‹ç¬¬ä¸€/ç¬¬äºŒä¸ª

model = VGG16_S(num_classes=total_classes_num).to(device) 
model.load_state_dict(torch.load(model_path))

def get_real_label(class_index):
    return label_dict[class_index]

def eval_predict(model, image_path):
    image = read_image(image_path, mode=torchvision.io.image.ImageReadMode.RGB)
    inner_transform = transforms.Compose(
    [
        transforms.Resize((32, 32)), 
        transforms.ColorJitter(
            brightness=0.1,
            contrast=0.1,
            saturation=0.1,
            hue=0.1),  # æŠ–åŠ¨å›¾åƒçš„äº®åº¦ã€å¯¹æ¯”åº¦ã€é¥±å’Œåº¦å’Œè‰²ç›¸
        transforms.Lambda(lambda x: x.float()),
        transforms.Normalize(
            global_mean,
            global_std
        )  # å¯¹å›¾ç‰‡æ•°æ®åšæ­£åˆ™åŒ–
    ])
    model.eval()
    timg = inner_transform(image)
    timg = timg.to(device)
    timg1 = timg.unsqueeze(0)
    result = model(timg1)
    _, predicted = torch.max(result, 1)
    get_real_label(predicted.item())
    return result, predicted```

## éªŒè¯ç»“æœå¯è§†åŒ–   

æ¨ç†, å¹¶ä¸”å¾—åˆ°å‰10ä¸ªæœ€åƒçš„ç±»å‹, ä¼˜åŒ–ç”¨æˆ·ä½“éªŒ. å› ä¸ºè®­ç»ƒæ•°æ®æœ‰é™, æ‰€ä»¥æ‰ç”¨å¢åŠ å¬å›æ•°é‡å’Œæ˜¾ç¤ºå„ä¸ªç§ç±»è¯†åˆ«ç‡çš„å½¢å¼ç»™ç”¨æˆ·é€‰æ‹©.æ¯•ç«Ÿ,æœ¬è´¨æ˜¯æ–¹ä¾¿ç”¨æˆ·è¯†åˆ«é“å…·ç±»å‹.  
```python
import torch.nn.functional as F
from PIL import Image, ImageDraw, ImageFont

real_test_images = [i for i in os.listdir("cus_test_data/") if i.find(".") != 0]
print(len(real_test_images))

for t_image_path in real_test_images:
    print("real label: ", image_path.split("/")[-1])    
    
    t_image_path = os.path.join("cus_test_data", t_image_path)
    t_image = Image.open(t_image_path)
    t_image = t_image.resize((32, 32))
    result, predicted = eval_predict(model, t_image_path)

    probabilities = F.softmax(result, dim=1)  
    values, indices = torch.topk(probabilities, 10)
    match_images = [t_image]
    for p, i in zip(values[0].tolist(), indices[0].tolist()):
        data = get_real_label(i)
        print(f"{data['en']} {data['zh']}: {round(100*p, 2)}%")
        match_img = Image.open(data.get("image"))
        # åˆ›å»ºä¸€ä¸ªå¯ä»¥åœ¨ç»™å®šå›¾åƒä¸Šç»˜å›¾çš„å¯¹è±¡
        draw = ImageDraw.Draw(match_img)
        # å­—ä½“é¢œè‰²
        font_color = (232,98,85)
        # ç»˜åˆ¶æ–‡æœ¬
        draw.text((0, 0), f"{round(100*p, 1)}%", fill=font_color)
        match_images.append(match_img)
        
    plot_images(match_images, figsize=(10, 10))
    print()
    print("----------------------------------------------")
```

![](pic/è‡ªå®šä¹‰è®­ç»ƒæ•°æ®å›¾åƒè¯†åˆ«-éªŒè¯çœŸå®ç»“æœ-2024-01-29.png)
![](pic/è‡ªå®šä¹‰è®­ç»ƒæ•°æ®å›¾åƒè¯†åˆ«-ç»“æœå¯è§†åŒ–-2024-01-29.png)
![](pic/è‡ªå®šä¹‰è®­ç»ƒæ•°æ®å›¾åƒè¯†åˆ«-ç»“æœæ€»ç»“-2024-01-29.png)   

# æ¢ç”¨ResNetç½‘ç»œè®­ç»ƒå¯¹æ¯”  
```python
%%time
# è®­ç»ƒæ¨¡å‹

# åŠ è½½ResNet101æ¨¡å‹
net = torchvision.models.resnet101(pretrained=True)
num_features = net.fc.in_features

# å¢åŠ  dropout
net.fc = torch.nn.Sequential(
    torch.nn.Dropout(0.5),   # å¢åŠ  dropouté¿å…å¤ªå¿«è¿‡æ‹Ÿåˆ
    torch.nn.Linear(num_features, total_classes_num)
)
# å•å±‚å·ç§¯, æ›¿ä»£äº†vggçš„4096ä¸ªå…¨è¿æ¥å±‚,æ·±,ä½†æ˜¯é€Ÿåº¦è¿˜æ›´å¿«
net.to(device)

model.eval()  # éªŒè¯æ¨¡å—

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.0001, weight_decay=0.01)  # åŠ å…¥äº†l2æ­£åˆ™åŒ–

check_iter = 10 # train check batch size 
train_epoch = 25
prefix = f"20240201_full_32x32_clear_{total_classes_num}class_l2_2-"

print("batch_size", batch_size)
for epoch in range(train_epoch):
    net.train()  # æ¯ä¸ªepoch ååˆ‡æ¢è®­ç»ƒæ¨¡å¼, é‚£ä¹ˆä¼šä¸ä¼šä¿dç•™ä¹‹å‰çš„è®­ç»ƒæƒé‡å‘¢?
    
    progress_bar = tqdm(enumerate(train_dataloader, 0), total=len(train_dataloader))
    for i, data in progress_bar:
        inputs, labels = data  # å¿…é¡»è¦float å½’ä¸€åŒ–? æµ®ç‚¹ç±»å‹.
        inputs, labels = inputs.float().to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = net(inputs)  # è¿™ä¸€æ­¥, è¿è¡Œæœ‰é—®é¢˜, è¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢, æ£€æŸ¥ä¸€ä¸‹å›¾ç‰‡æ ¼å¼

        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        progress_bar.set_description(f'epoch:{epoch}, loss: {loss / batch_size:.3f}')
    
    if epoch % 1 == 0:  # æ¯ä¸¤ä¸ªepochè¿›è¡Œä¸€æ¬¡éªŒè¯
        print("å¼€å§‹éªŒè¯....")
        net.eval()
        correct = 0  # è®°å½•æ­£ç¡®é¢„æµ‹çš„æ•°é‡
        total = 0  # æ€»çš„æ ·æœ¬æ•°
        with torch.no_grad():
            
            progress_bar2 = tqdm(enumerate(train_dataloader, 0), total=int(len(train_dataloader) * 0.4))
            for i, data in progress_bar2:
                if i >= int(len(train_dataloader) * 0.4):
                    break
                inputs, labels = data
                inputs, labels = inputs.float().to(device), labels.to(device)
                outputs = net(inputs)
                _, predicted = torch.max(outputs.data, 1)

                total += labels.size(0)  # å®é™…çš„æ ·æœ¬æ•°
                correct += (predicted == labels).sum().item()  # æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬æ•°
        
        accuracy = round(correct / total * 100, 3)  # è®¡ç®—å‡†ç¡®ç‡
        print(f'epoch: {epoch}, Accuracy: {accuracy:.3f}')
        if accuracy >= 60:  # å¤ªå°çš„æ ¹æœ¬æ²¡æ¯”è¾ƒä¿å­˜
            save_model_path = f"{prefix}_resnet101_{epoch}_{accuracy}.pth"
            torch.save(net.state_dict(), save_model_path)
            print(f"model save: ", save_model_path)
        
    torch.cuda.empty_cache()
print('Finished Training')

save_model_path = f"{prefix}_resnet101_{epoch}_{accuracy}.pth"
torch.save(net.state_dict(), save_model_path)
print(f"model save: ", save_model_path)


```

## ResNetæ¨¡å‹éªŒè¯  
å¾—ç›Šäº`ResNet`é‡‡ç”¨äº†è·³è·ƒé“¾æ¥, æŸå¤±å€¼èƒ½å¤Ÿæ›´å¥½çš„åœ¨æ›´æ·±çš„ç½‘ç»œä¸­è¿›è¡Œä¼ é€’, è¡¨ç°çš„æ•ˆæœå°±æ˜¯è®­ç»ƒæ›´å®¹æ˜“æ”¶æ•›äº†.

è®­ç»ƒçš„è¾“å‡ºå¦‚ä¸‹, å¯ä»¥çœ‹åˆ°resnetçš„æ”¶æ•›é€Ÿåº¦æå¿«, 10ä¸ªepochä»¥å†…å°±å¯ä»¥è¾¾åˆ°90%çš„å‡†ç¡®ç‡,, è€ŒvggNetåˆ™è¦æ¥è¿‘10ä¸ªepochæ˜¯è¿œè¿œæ²¡æ³•è¾¾åˆ°90%å‡†ç¡®ç‡çš„.  
ResNetå‡†ç¡®æ€§æ›´é«˜, æ‰€ä»¥å¦‚æœæ˜¯å®é™…åº”ç”¨äº§å“çš„è¯, å¯ä»¥ç›´æ¥ä¸ç”¨ç®¡å¤è€çš„å†å²åŒ…è¢±äº†, vggå½“ç„¶ä¹Ÿæœ‰å®ƒçš„æ ‡å¿—æ€§æ„ä¹‰. ä½†æ˜¯çœŸå®çœäº‹, ç¡®å®ç›´æ¥é€‰æ¨¡å‹æŒ‘é€‰æ–°ä¸€ç‚¹çš„, ä¼šå°‘ä¸€äº›å‘. ğŸ¤” 
```shell
epoch:4, loss: 0.005: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [02:45<00:00,  1.95s/it]

å¼€å§‹éªŒè¯....

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:17<00:00,  1.90it/s]

epoch: 4, Accuracy: 96.507
model save:  20240201_full_32x32_clear_906class_l2_2-_resnet101_4_96.507.pth
```

è½½å…¥è®­ç»ƒåæ¨¡å‹, ä½¿ç”¨ä¸Šé¢vgg16éªŒè¯çš„ä»£ç , è¿›è¡ŒéªŒè¯, å¯¹æ¯”åå‘ç°, æ•´ä½“çš„æ³›åŒ–èƒ½åŠ›, ResNetè¦æ›´å¥½, å¾ˆå¤šVggNetæ²¡èƒ½ç¨³å®šè¯†åˆ«çš„éªŒè¯å›¾ç‰‡, ResNetéƒ½èƒ½æ¯”è¾ƒç¨³å®šçš„ä»¥æ¯”è¾ƒé«˜çš„è¯†åˆ«ç‡è¿›è¡Œè¯†åˆ«.  



# æ€»ç»“   

ä¸€æ—¶å…´èµ·çš„å°éœ€æ±‚, ä¹Ÿå¾ˆå¤šå°ç»†èŠ‚éœ€è¦å¤„ç†. 
å¤§æ‰¹é‡æ•°æ®è¿è¡Œå‰, éœ€è¦å…ˆæŠ½å°‘ä¸€äº›ç±»è¿›è¡Œå°æ‰¹é‡çš„å¿«é€ŸéªŒè¯, å¯¹æ¨¡å‹å’Œè®­ç»ƒæ•°æ®è¿›è¡ŒéªŒè¯. è¿™æ¬¡æˆ‘å°±æ˜¯å…ˆç”¨20ç±»è¿›è¡ŒéªŒè¯, ä¹‹åæ‰è¿›è¡Œ900å¤šç±»çš„è®­ç»ƒ.   

ä¸€å¼€å§‹ç”¨vgg, ç”¨çš„æ˜¯224x224çš„å›¾ç‰‡è¾“å…¥, åé¢ä¸ºäº†æé€Ÿ.å› ä¸ºæ˜¯åƒç´ ç”»,åªè¦ä¸è¦å¹³æ»‘, 32x32ä¹Ÿè¶³å¤Ÿæ¸…æ™°, æ‰€ä»¥å°±è°ƒæ•´vggç»“æ„, æ–¹ä¾¿è¿›è¡Œå¢åŠ æ›´å¤šçš„batch_sizeè¿›è¡Œæ›´å¿«çš„è®­ç»ƒ.åŠæ—¶è°ƒæ•´äº†è¾“å…¥å›¾ç‰‡çš„å°ºå¯¸ä»¥å, æˆ‘çš„ç”µè„‘è®­ç»ƒèµ·æ¥ä¹Ÿå¿«äº†å¾ˆå¤š, 40å¤šåˆ†é’Ÿå°±å¯ä»¥å®Œæˆ25è½® 128batch_sizeçš„æ¨¡å‹è®­ç»ƒ, ä½“éªŒè¿˜ä¸é”™!    

ç„¶åå°±æ˜¯, å¯ä»¥çš„è¯, ä¸»åŠ¨æŒ‘é€‰æ›´æ–°çš„ä»£ç , è€çš„æ¨¡å‹, ç»å…¸, ä½†æ˜¯æ•ˆæœè¿œä¸å¦‚æ–°ä¸€äº›çš„æ¨¡å‹.ResNetæ›´å¿«æ”¶æ•›, æ›´å¿«è®­ç»ƒ(ç¬‘...)

[æ¨¡å‹è®­ç»ƒå®Œæ•´ä»£ç ](https://github.com/realzhengyiming/isaac_tools/blob/main/simple_issac_item_recognition-%E5%85%A8%E9%87%8F%E8%AE%AD%E7%BB%83%E7%9A%84%E7%89%88%E6%9C%AC.ipynb)  
[æ•°æ®è·å–å’Œå¤„ç†çš„å®Œæ•´ä»£ç ](https://github.com/realzhengyiming/isaac_tools/blob/main/isaac_data_spider.py)
ä»£ç éƒ½åœ¨githubä¸Šäº†.  
æˆ‘è¦å»ç©æ¸¸æˆäº†, ä»¥æ’’çœŸå¥½ç© : ) 
## åç»­ä¼˜åŒ–    

+ å¢åŠ ä¸€ä¸‹çœŸå®çš„æˆªå›¾ä½œä¸ºè®­ç»ƒæ•°æ®, ä»¥è¿›ä¸€æ­¥æé«˜å„ä¸ªç±»åˆ«çš„å‡†ç¡®åº¦
+ è¿›ä¸€æ­¥å¢åŠ ä¸€äº›å…¶ä»–çš„æ•°æ®å¢å¼º, ä»¥æ›´å¥½çš„æ¨¡æ‹ŸçœŸå®çš„ä½¿ç”¨æƒ…å†µ
- [x] è€ƒè™‘æ¢ç”¨å…¶ä»–æ¨¡å‹(resnetæ›¿æ¢éªŒè¯, æ”¹è¿›çš„æ¨¡å‹æ˜æ˜¾æ›´å¥½, å†å²åŒ…è¢±åº”è¯¥æ—©æ—¥ä¸¢å¼ƒ.)
+ éƒ¨ç½²æ¨¡å‹åˆ°ç§»åŠ¨ç«¯, å°è£…æˆåº”ç”¨è¿›è¡Œä½¿ç”¨, æ”¹è®­ç»ƒç›®æ ‡æ£€æµ‹æ¨¡å‹.

